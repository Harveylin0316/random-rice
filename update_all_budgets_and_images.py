#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›´æ–°æ‰€æœ‰é¤å»³çš„é ç®—å’Œç…§ç‰‡
1. å¾ OpenRice çˆ¬å–åƒ¹æ ¼è³‡è¨Š
2. å¾ OpenRice çˆ¬å–ç…§ç‰‡
"""

import json
import requests
from bs4 import BeautifulSoup
import re
import time
from typing import List, Optional, Dict

def scrape_price_from_openrice(url: str) -> Optional[str]:
    """
    å¾ OpenRice URL çˆ¬å–é¤å»³åƒ¹æ ¼è³‡è¨Š
    è¿”å›åƒ¹æ ¼å€é–“å­—ä¸²ï¼Œå¦‚ "500-800"
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Referer': 'https://www.openrice.com/',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        all_text = soup.get_text()
        
        # æª¢æŸ¥æ˜¯å¦å·²çµæ¥­
        if any(keyword in all_text for keyword in ['å·²çµæ¥­', 'å·²æ­‡æ¥­', 'å·²åœæ¥­']):
            return None
        
        # å„ªå…ˆæŸ¥æ‰¾ |NT$XXX-XXX| ç¯„åœæ ¼å¼
        pipe_range_match = re.search(r'\|NT\$(\d+)\s*[-~è‡³]\s*NT\$(\d+)\|', all_text)
        if not pipe_range_match:
            pipe_range_match = re.search(r'\|NT\$(\d+)\s*[-~è‡³]\s*(\d+)\|', all_text)
        if pipe_range_match:
            min_price = int(pipe_range_match.group(1))
            max_price = int(pipe_range_match.group(2))
            return convert_to_budget_range(min_price, max_price)
        
        # æŸ¥æ‰¾ |NT$XXXä»¥ä¸Š| æ ¼å¼
        pipe_above_match = re.search(r'\|NT\$(\d+)ä»¥ä¸Š\|', all_text)
        if pipe_above_match:
            price_num = int(pipe_above_match.group(1))
            return convert_to_budget_range(price_num, price_num * 2)
        
        # æŸ¥æ‰¾ |NT$XXX| å–®ä¸€åƒ¹æ ¼æ ¼å¼
        pipe_single_match = re.search(r'\|NT\$(\d+)\|', all_text)
        if pipe_single_match:
            price_num = int(pipe_single_match.group(1))
            return convert_to_budget_range(price_num, price_num * 1.5)
        
        # æŸ¥æ‰¾ NT$XXX-XXX æ ¼å¼ï¼ˆä¸åœ¨ | | ä¸­ï¼‰
        nt_range_match = re.search(r'NT\$(\d+)\s*[-~è‡³]\s*NT\$(\d+)', all_text)
        if not nt_range_match:
            nt_range_match = re.search(r'NT\$(\d+)\s*[-~è‡³]\s*(\d+)', all_text)
        if nt_range_match:
            min_price = int(nt_range_match.group(1))
            max_price = int(nt_range_match.group(2))
            return convert_to_budget_range(min_price, max_price)
        
        # æŸ¥æ‰¾å…¶ä»–åƒ¹æ ¼æ ¼å¼
        price_patterns = [
            r'äººå‡[æ¶ˆè²»]*[ï¼š:]?\s*[\$NT]*\s*(\d+)\s*[-~è‡³]\s*[\$NT]*\s*(\d+)',
            r'å¹³å‡[æ¶ˆè²»]*[ï¼š:]?\s*[\$NT]*\s*(\d+)\s*[-~è‡³]\s*[\$NT]*\s*(\d+)',
            r'æ¶ˆè²»[ï¼š:]?\s*[\$NT]*\s*(\d+)\s*[-~è‡³]\s*[\$NT]*\s*(\d+)',
            r'(\d+)\s*[-~è‡³]\s*(\d+)\s*[å…ƒ]*',
        ]
        
        for pattern in price_patterns:
            match = re.search(pattern, all_text)
            if match:
                min_price = int(match.group(1))
                max_price = int(match.group(2)) if len(match.groups()) > 1 else min_price * 1.5
                return convert_to_budget_range(min_price, max_price)
        
        return None
        
    except Exception as e:
        print(f"    åƒ¹æ ¼çˆ¬å–éŒ¯èª¤ï¼š{str(e)}")
        return None

def convert_to_budget_range(min_price: int, max_price: int) -> str:
    """å°‡åƒ¹æ ¼è½‰æ›ç‚ºé ç®—å€é–“æ ¼å¼"""
    avg_price = (min_price + max_price) / 2
    
    if avg_price < 200:
        return "200ä»¥ä¸‹"
    elif avg_price < 400:
        return "200-400"
    elif avg_price < 800:
        return "500-800"
    elif avg_price < 1500:
        return "1000-1500"
    elif avg_price < 2000:
        return "1500-2000"
    else:
        return "2000ä»¥ä¸Š"

def scrape_images_from_openrice(url: str) -> List[str]:
    """
    å¾ OpenRice URL çˆ¬å–é¤å»³ç…§ç‰‡
    è¿”å›ç…§ç‰‡ URL åˆ—è¡¨
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Referer': 'https://www.openrice.com/',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        image_urls = []
        
        # æŸ¥æ‰¾æ‰€æœ‰ img æ¨™ç±¤
        img_tags = soup.find_all('img')
        
        for img in img_tags:
            img_url = img.get('data-src') or img.get('src') or img.get('data-lazy-src') or img.get('data-original')
            
            if img_url:
                # ç¢ºä¿æ˜¯å®Œæ•´çš„ URL
                if img_url.startswith('http'):
                    if img_url not in image_urls:
                        image_urls.append(img_url)
                elif img_url.startswith('//'):
                    img_url = 'https:' + img_url
                    if img_url not in image_urls:
                        image_urls.append(img_url)
        
        # éæ¿¾ç…§ç‰‡ URL
        filtered_urls = []
        exclude_keywords = ['logo', 'icon', 'avatar', 'profile', 'button', 'arrow', 'close', 'spinner', 'loading']
        
        for url in image_urls:
            url_lower = url.lower()
            
            # è·³éæ˜é¡¯ä¸æ˜¯ç…§ç‰‡çš„ URL
            if any(keyword in url_lower for keyword in exclude_keywords):
                continue
            
            # å„ªå…ˆä¿ç•™ OpenRice CDN çš„ç…§ç‰‡
            if 'orstatic.com' in url_lower or 'openrice.com' in url_lower:
                # ç¢ºä¿æ˜¯åœ–ç‰‡æ ¼å¼
                if any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp', 'photo', 'image']):
                    # è·³éå¤ªå°çš„åœ–ç‰‡ï¼Œé€šå¸¸ç…§ç‰‡ URL æœƒåŒ…å«å°ºå¯¸æ¨™è¨˜
                    if any(size in url_lower for size in ['px', 'mx', 'large', 'medium', 'thumb']) or 'userphoto' in url_lower or 'photo' in url_lower:
                        filtered_urls.append(url)
            else:
                # å…¶ä»–ä¾†æºçš„åœ–ç‰‡
                if any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    if any(keyword in url_lower for keyword in ['photo', 'image', 'picture', 'food', 'dish', 'restaurant']):
                        filtered_urls.append(url)
        
        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        unique_urls = []
        seen = set()
        for url in filtered_urls:
            if url not in seen:
                unique_urls.append(url)
                seen.add(url)
                if len(unique_urls) >= 10:
                    break
        
        return unique_urls
        
    except Exception as e:
        print(f"    ç…§ç‰‡çˆ¬å–éŒ¯èª¤ï¼š{str(e)}")
        return []

def update_all_restaurants(file_path: str, update_budget: bool = True, update_images: bool = True, test_mode: bool = False, limit: int = 10):
    """
    æ›´æ–°æ‰€æœ‰é¤å»³çš„é ç®—å’Œç…§ç‰‡
    """
    print("=" * 80)
    print("é–‹å§‹æ›´æ–°é¤å»³è³‡æ–™...")
    print("=" * 80)
    
    # è¼‰å…¥è³‡æ–™åº«
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    restaurants = data['restaurants']
    
    if test_mode:
        print(f"\nâš ï¸  æ¸¬è©¦æ¨¡å¼ï¼šåªè™•ç†å‰ {limit} é–“é¤å»³")
        restaurants_to_process = restaurants[:limit]
    else:
        restaurants_to_process = restaurants
    
    print(f"\nç¸½å…±é¤å»³æ•¸ï¼š{len(restaurants_to_process)} é–“")
    
    # çµ±è¨ˆ
    no_budget_count = sum(1 for r in restaurants_to_process if not r.get('budget'))
    no_images_count = sum(1 for r in restaurants_to_process if not r.get('images') or len(r.get('images', [])) == 0)
    
    print(f"  ç„¡é ç®—ï¼š{no_budget_count} é–“")
    print(f"  ç„¡ç…§ç‰‡ï¼š{no_images_count} é–“")
    print("\né–‹å§‹è™•ç†...\n")
    
    budget_updated = 0
    images_updated = 0
    failed_budget = 0
    failed_images = 0
    
    for i, restaurant in enumerate(restaurants_to_process, 1):
        name = restaurant.get('name', 'æœªçŸ¥é¤å»³')
        url = restaurant.get('url', '')
        
        print(f"[{i}/{len(restaurants_to_process)}] {name[:40]}")
        
        if not url:
            print(f"  âŒ ç„¡ URLï¼Œè·³é")
            continue
        
        # æ›´æ–°é ç®—
        if update_budget and not restaurant.get('budget'):
            print(f"  æ­£åœ¨çˆ¬å–åƒ¹æ ¼...", end=' ')
            budget = scrape_price_from_openrice(url)
            if budget:
                restaurant['budget'] = budget
                print(f"âœ… {budget}")
                budget_updated += 1
            else:
                print(f"âŒ æœªæ‰¾åˆ°")
                failed_budget += 1
            time.sleep(0.5)  # é¿å…è«‹æ±‚éå¿«
        
        # æ›´æ–°ç…§ç‰‡
        if update_images and (not restaurant.get('images') or len(restaurant.get('images', [])) == 0):
            print(f"  æ­£åœ¨çˆ¬å–ç…§ç‰‡...", end=' ')
            images = scrape_images_from_openrice(url)
            if images:
                restaurant['images'] = images
                print(f"âœ… {len(images)} å¼µ")
                images_updated += 1
            else:
                print(f"âŒ æœªæ‰¾åˆ°")
                restaurant['images'] = []  # æ¨™è¨˜ç‚ºå·²è™•ç†
                failed_images += 1
            time.sleep(0.5)  # é¿å…è«‹æ±‚éå¿«
        
        # æ¯è™•ç† 10 é–“é¤å»³ä¿å­˜ä¸€æ¬¡ï¼ˆé˜²æ­¢æ•¸æ“šä¸Ÿå¤±ï¼‰
        if i % 10 == 0:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"  ğŸ’¾ å·²ä¿å­˜é€²åº¦ï¼ˆ{i}/{len(restaurants_to_process)}ï¼‰")
    
    # æœ€çµ‚ä¿å­˜
    print("\n" + "=" * 80)
    print("ä¿å­˜æ›´æ–°å¾Œçš„è³‡æ–™åº«...")
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 80)
    print("âœ… è™•ç†å®Œæˆï¼")
    print("\nçµ±è¨ˆï¼š")
    print(f"  ç¸½é¤å»³æ•¸ï¼š{len(restaurants_to_process)} é–“")
    if update_budget:
        print(f"  é ç®—æ›´æ–°ï¼š{budget_updated} é–“")
        print(f"  é ç®—å¤±æ•—ï¼š{failed_budget} é–“")
    if update_images:
        print(f"  ç…§ç‰‡æ›´æ–°ï¼š{images_updated} é–“")
        print(f"  ç…§ç‰‡å¤±æ•—ï¼š{failed_images} é–“")
    print("=" * 80)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ›´æ–°æ‰€æœ‰é¤å»³çš„é ç®—å’Œç…§ç‰‡")
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼ï¼šåªè™•ç†å‰ 10 é–“é¤å»³')
    parser.add_argument('--limit', type=int, default=10, help='æ¸¬è©¦æ¨¡å¼ä¸‹è™•ç†çš„é¤å»³æ•¸é‡ï¼ˆé è¨­ï¼š10ï¼‰')
    parser.add_argument('--budget-only', action='store_true', help='åªæ›´æ–°é ç®—')
    parser.add_argument('--images-only', action='store_true', help='åªæ›´æ–°ç…§ç‰‡')
    
    args = parser.parse_args()
    
    update_budget = not args.images_only
    update_images = not args.budget_only
    
    update_all_restaurants(
        'restaurants_database.json',
        update_budget=update_budget,
        update_images=update_images,
        test_mode=args.test,
        limit=args.limit
    )
