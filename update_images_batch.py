#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æ›´æ–°é¤å»³ç…§ç‰‡ï¼ˆåˆ†æ‰¹è™•ç†ï¼‰
"""

import json
import requests
from bs4 import BeautifulSoup
import time
from typing import List

def scrape_images_from_openrice(url: str) -> List[str]:
    """å¾ OpenRice URL çˆ¬å–é¤å»³ç…§ç‰‡"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Referer': 'https://www.openrice.com/',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        image_urls = []
        
        # æŸ¥æ‰¾æ‰€æœ‰ img æ¨™ç±¤
        img_tags = soup.find_all('img')
        
        for img in img_tags:
            img_url = img.get('data-src') or img.get('src') or img.get('data-lazy-src') or img.get('data-original')
            
            if img_url:
                if img_url.startswith('http'):
                    if img_url not in image_urls:
                        image_urls.append(img_url)
                elif img_url.startswith('//'):
                    img_url = 'https:' + img_url
                    if img_url not in image_urls:
                        image_urls.append(img_url)
        
        # éæ¿¾ç…§ç‰‡ URL
        filtered_urls = []
        exclude_keywords = ['logo', 'icon', 'avatar', 'profile', 'button', 'arrow', 'close', 'spinner', 'loading']
        
        for url in image_urls:
            url_lower = url.lower()
            
            if any(keyword in url_lower for keyword in exclude_keywords):
                continue
            
            # å„ªå…ˆä¿ç•™ OpenRice CDN çš„ç…§ç‰‡
            if 'orstatic.com' in url_lower or 'openrice.com' in url_lower:
                if any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp', 'photo', 'image']):
                    if any(size in url_lower for size in ['px', 'mx', 'large', 'medium', 'thumb']) or 'userphoto' in url_lower or 'photo' in url_lower:
                        filtered_urls.append(url)
            else:
                if any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    if any(keyword in url_lower for keyword in ['photo', 'image', 'picture', 'food', 'dish', 'restaurant']):
                        filtered_urls.append(url)
        
        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        unique_urls = []
        seen = set()
        for url in filtered_urls:
            if url not in seen:
                unique_urls.append(url)
                seen.add(url)
                if len(unique_urls) >= 10:
                    break
        
        return unique_urls
        
    except Exception as e:
        return []

def update_images_batch(file_path: str, batch_size: int = 50, start_index: int = 0):
    """æ‰¹é‡æ›´æ–°é¤å»³ç…§ç‰‡"""
    print("=" * 80)
    print("é–‹å§‹æ‰¹é‡æ›´æ–°é¤å»³ç…§ç‰‡...")
    print("=" * 80)
    
    # è¼‰å…¥è³‡æ–™åº«
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æ‰¾å‡ºæ‰€æœ‰æ²’æœ‰ç…§ç‰‡çš„é¤å»³
    no_images_restaurants = [(i, r) for i, r in enumerate(data['restaurants']) 
                             if not r.get('images') or len(r.get('images', [])) == 0]
    
    print(f"\nç¸½å…±ç„¡ç…§ç‰‡é¤å»³ï¼š{len(no_images_restaurants)} é–“")
    print(f"æœ¬æ¬¡è™•ç†ï¼šå¾ç¬¬ {start_index + 1} é–“é–‹å§‹ï¼Œè™•ç† {batch_size} é–“\n")
    
    # è™•ç†æŒ‡å®šç¯„åœçš„é¤å»³
    if start_index >= len(no_images_restaurants):
        print(f"âš ï¸  èµ·å§‹ç´¢å¼• {start_index} è¶…å‡ºç¯„åœï¼ˆç¸½å…± {len(no_images_restaurants)} é–“ç„¡ç…§ç‰‡é¤å»³ï¼‰")
        return
    
    end_index = min(start_index + batch_size, len(no_images_restaurants))
    restaurants_to_process = no_images_restaurants[start_index:end_index]
    
    updated_count = 0
    failed_count = 0
    
    for idx, (original_idx, restaurant) in enumerate(restaurants_to_process, 1):
        name = restaurant.get('name', 'æœªçŸ¥é¤å»³')
        url = restaurant.get('url', '')
        
        current_num = start_index + idx
        print(f"[{current_num}/{len(no_images_restaurants)}] {name[:50]}")
        
        if not url:
            print(f"  âŒ ç„¡ URLï¼Œè·³é")
            failed_count += 1
            continue
        
        # çˆ¬å–ç…§ç‰‡
        images = scrape_images_from_openrice(url)
        
        if images:
            data['restaurants'][original_idx]['images'] = images
            print(f"  âœ… {len(images)} å¼µ")
            updated_count += 1
        else:
            print(f"  âŒ æœªæ‰¾åˆ°ç…§ç‰‡")
            data['restaurants'][original_idx]['images'] = []  # æ¨™è¨˜ç‚ºå·²è™•ç†
            failed_count += 1
        
        # æ¯è™•ç† 10 é–“ä¿å­˜ä¸€æ¬¡
        if idx % 10 == 0:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"  ğŸ’¾ å·²ä¿å­˜é€²åº¦")
        
        # é¿å…è«‹æ±‚éå¿«
        time.sleep(0.8)
    
    # æœ€çµ‚ä¿å­˜
    print("\n" + "=" * 80)
    print("ä¿å­˜æ›´æ–°å¾Œçš„è³‡æ–™åº«...")
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 80)
    print("âœ… æœ¬æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
    print(f"\nçµ±è¨ˆï¼š")
    print(f"  è™•ç†æ•¸é‡ï¼š{len(restaurants_to_process)} é–“")
    print(f"  æˆåŠŸæ›´æ–°ï¼š{updated_count} é–“")
    print(f"  å¤±æ•—ï¼š{failed_count} é–“")
    if len(restaurants_to_process) > 0:
        print(f"  æˆåŠŸç‡ï¼š{updated_count/len(restaurants_to_process)*100:.1f}%")
    print("=" * 80)
    
    # æç¤ºä¸‹ä¸€æ‰¹æ¬¡
    if end_index < len(no_images_restaurants):
        print(f"\nğŸ’¡ æç¤ºï¼šé‚„æœ‰ {len(no_images_restaurants) - end_index} é–“é¤å»³å¾…è™•ç†")
        print(f"   ä¸‹æ¬¡é‹è¡Œï¼špython3 update_images_batch.py --start {end_index}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ‰¹é‡æ›´æ–°é¤å»³ç…§ç‰‡")
    parser.add_argument('--batch-size', type=int, default=50, help='æ¯æ‰¹è™•ç†çš„é¤å»³æ•¸é‡ï¼ˆé è¨­ï¼š50ï¼‰')
    parser.add_argument('--start', type=int, default=0, help='å¾ç¬¬å¹¾é–“é–‹å§‹è™•ç†ï¼ˆé è¨­ï¼š0ï¼‰')
    
    args = parser.parse_args()
    
    update_images_batch(
        'restaurants_database.json',
        batch_size=args.batch_size,
        start_index=args.start
    )
